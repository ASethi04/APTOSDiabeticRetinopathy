{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../input/efficientnet/efficientnet-master/efficientnet-master/'))\n",
    "from efficientnet import EfficientNetB5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Files and file sizes\n",
      "sample_submission.csv         | 0.03 MB\n",
      "train.csv                     | 0.05 MB\n",
      "test.csv                      | 0.03 MB\n",
      "train_images                  | 0.14 MB\n",
      "test_images                   | 0.07 MB\n",
      "\n",
      "# Files and file sizes\n",
      "sample_submission.csv         | 0.03 MB\n",
      "train.csv                     | 0.05 MB\n",
      "test.csv                      | 0.03 MB\n",
      "train_images                  | 0.14 MB\n",
      "test_images                   | 0.07 MB\n"
     ]
    }
   ],
   "source": [
    "# Standard dependencies\n",
    "import cv2\n",
    "import time\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from numpy import expand_dims\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from PIL import ImageOps\n",
    "import os\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.applications import Xception\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.applications import xception\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.activations import elu\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.layers import Dense, Conv2D, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Standard dependencies\n",
    "import cv2\n",
    "import time\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine Learning\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.activations import elu\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.layers import Dense, Conv2D, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Path specifications\n",
    "KAGGLE_DIR = '../input/aptos2019-blindness-detection/'\n",
    "TRAIN_DF_PATH = KAGGLE_DIR + \"train.csv\"\n",
    "TEST_DF_PATH = KAGGLE_DIR + 'test.csv'\n",
    "TRAIN_IMG_PATH = KAGGLE_DIR + \"train_images/\"\n",
    "TEST_IMG_PATH = KAGGLE_DIR + 'test_images/'\n",
    "\n",
    "# Specify title of our final model\n",
    "SAVED_MODEL_NAME = 'effnet_modelB5.h5'\n",
    "\n",
    "# Set seed for reproducability\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "\n",
    "# For keeping time. GPU limit for this competition is set to Â± 9 hours.\n",
    "t_start = time.time()\n",
    "\n",
    "# File sizes and specifications\n",
    "print('\\n# Files and file sizes')\n",
    "for file in os.listdir(KAGGLE_DIR):\n",
    "    print('{}| {} MB'.format(file.ljust(30), \n",
    "                             str(round(os.path.getsize(KAGGLE_DIR + file) / 1000000, 2))))\n",
    "\n",
    "# File sizes and specifications\n",
    "print('\\n# Files and file sizes')\n",
    "for file in os.listdir(KAGGLE_DIR):\n",
    "    print('{}| {} MB'.format(file.ljust(30), \n",
    "                             str(round(os.path.getsize(KAGGLE_DIR + file) / 1000000, 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/aptos2019-blindness-detection/train.csv')\n",
    "test_df = pd.read_csv('../input/aptos2019-blindness-detection/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify image size\n",
    "IMG_WIDTH = 600\n",
    "IMG_HEIGHT = 600\n",
    "CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds_and_labels(model, generator):\n",
    "    \"\"\"\n",
    "    Get predictions and labels from the generator\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    labels = []\n",
    "    for _ in range(generator.samples // BATCH_SIZE):\n",
    "        x, y = next(generator)\n",
    "        preds.append(model.predict(x))\n",
    "        labels.append(y)\n",
    "    # Flatten list of numpy arrays\n",
    "    return np.concatenate(preds).ravel(), np.concatenate(labels).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "\n",
    "    \"\"\"\n",
    "    A custom Keras callback for saving the best model\n",
    "    according to the Quadratic Weighted Kappa (QWK) metric\n",
    "    \"\"\"\n",
    "    def on_train_begin(self, logs={}):\n",
    "        \"\"\"\n",
    "        Initialize list of QWK scores on validation data\n",
    "        \"\"\"\n",
    "        self.val_kappas = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"\n",
    "        Gets QWK score on the validation data\n",
    "        and saves the model if the score is better\n",
    "        than previous epochs\n",
    "        \"\"\"\n",
    "        # Get predictions and convert to integers\n",
    "        y_pred, labels = get_preds_and_labels(model, val_generator)\n",
    "        y_pred = np.rint(y_pred).astype(np.uint8).clip(0, 4)\n",
    "        # We can use sklearns implementation of QWK straight out of the box\n",
    "        # as long as we specify weights as 'quadratic'\n",
    "        _val_kappa = cohen_kappa_score(labels, y_pred, weights='quadratic')\n",
    "        self.val_kappas.append(_val_kappa)\n",
    "        print(f\"val_kappa: {_val_kappa:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if _val_kappa == max(self.val_kappas):\n",
    "            print(\"Validation Kappa has improved. Saving model.\")\n",
    "            self.model.save(SAVED_MODEL_NAME)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Label distribution\n",
    "train_df['diagnosis'].value_counts().sort_index().plot(kind=\"bar\", \n",
    "                                                       figsize=(12,5), \n",
    "                                                       rot=0)\n",
    "plt.title(\"Label Distribution (Training Set)\", \n",
    "          weight='bold', \n",
    "          fontsize=18)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xlabel(\"Label\", fontsize=17)\n",
    "plt.ylabel(\"Frequency\", fontsize=17);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image_from_gray(img, tol=7):\n",
    "    \"\"\"\n",
    "    Applies masks to the orignal image and \n",
    "    returns the a preprocessed image with \n",
    "    3 channels\n",
    "    \"\"\"\n",
    "    # If for some reason we only have two channels\n",
    "    if img.ndim == 2:\n",
    "        mask = img > tol\n",
    "        return img[np.ix_(mask.any(1),mask.any(0))]\n",
    "    # If we have a normal RGB images\n",
    "    elif img.ndim == 3:\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = gray_img > tol\n",
    "        \n",
    "        check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0]\n",
    "        if (check_shape == 0): # image is too dark so that we crop out everything,\n",
    "            return img # return original image\n",
    "        else:\n",
    "            img1=img[:,:,0][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img2=img[:,:,1][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img3=img[:,:,2][np.ix_(mask.any(1),mask.any(0))]\n",
    "            img = np.stack([img1,img2,img3],axis=-1)\n",
    "        return img\n",
    "\n",
    "def preprocess_image(path, sigmaX=10):\n",
    "    \"\"\"\n",
    "    The whole preprocessing pipeline:\n",
    "    1. Read in image\n",
    "    2. Apply masks\n",
    "    3. Resize image to desired size\n",
    "    4. Add Gaussian noise to increase Robustness\n",
    "    \"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = crop_image_from_gray(image)\n",
    "    image = cv2.resize(image, (IMG_WIDTH, IMG_HEIGHT))\n",
    "    image = cv2.addWeighted (image,4, cv2.GaussianBlur(image, (0,0) ,sigmaX), -4, 128)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image IDs and Labels (TRAIN)\n",
      "Training images: 3662\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000c1434d8d7.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001639a390f0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0024cdab0c1e.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002c21358ce6.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005b95c28852.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id_code  diagnosis\n",
       "0  000c1434d8d7.png          2\n",
       "1  001639a390f0.png          4\n",
       "2  0024cdab0c1e.png          1\n",
       "3  002c21358ce6.png          0\n",
       "4  005b95c28852.png          0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image IDs (TEST)\n",
      "Testing Images: 1928\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0005cfc8afb6.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003f0afdcd15.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>006efc72b638.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00836aaacf06.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>009245722fa4.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id_code\n",
       "0  0005cfc8afb6.png\n",
       "1  003f0afdcd15.png\n",
       "2  006efc72b638.png\n",
       "3  00836aaacf06.png\n",
       "4  009245722fa4.png"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../input/aptos2019-blindness-detection/train_images/\n",
      "Found 3113 validated image filenames.\n",
      "Found 549 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "print(\"Image IDs and Labels (TRAIN)\")\n",
    "train_df = pd.read_csv(TRAIN_DF_PATH)\n",
    "# Add extension to id_code\n",
    "train_df['id_code'] = train_df['id_code'] + \".png\"\n",
    "print(f\"Training images: {train_df.shape[0]}\")\n",
    "display(train_df.head())\n",
    "print(\"Image IDs (TEST)\")\n",
    "test_df = pd.read_csv(TEST_DF_PATH)\n",
    "# Add extension to id_code\n",
    "test_df['id_code'] = test_df['id_code'] + \".png\"\n",
    "print(f\"Testing Images: {test_df.shape[0]}\")\n",
    "display(test_df.head())\n",
    "y_labels = train_df['diagnosis'].values\n",
    "BATCH_SIZE = 32\n",
    "print(TRAIN_IMG_PATH)\n",
    "train_datagen = ImageDataGenerator(rotation_range=360,\n",
    "                                   horizontal_flip=True,\n",
    "                                   vertical_flip=True,\n",
    "                                   validation_split=0.15,\n",
    "                                   rescale=1 / 128)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(train_df, \n",
    "                                                    x_col='id_code', \n",
    "                                                    y_col='diagnosis',\n",
    "                                                    directory = TRAIN_IMG_PATH,\n",
    "                                                    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    class_mode='other',\n",
    "                                                    preprocessing_function=preprocess_image, \n",
    "                                                    subset='training')\n",
    "\n",
    "val_generator = train_datagen.flow_from_dataframe(train_df, \n",
    "                                                  x_col='id_code', \n",
    "                                                  y_col='diagnosis',\n",
    "                                                  directory = TRAIN_IMG_PATH,\n",
    "                                                  target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  class_mode='other',\n",
    "                                                  preprocessing_function=preprocess_image, \n",
    "                                                  subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b5 (Model)      (None, 19, 19, 2048)      28513520  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 28,515,569\n",
      "Trainable params: 28,342,833\n",
      "Non-trainable params: 172,736\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GaussianNoise\n",
    "\n",
    "# Load in EfficientNetB3\n",
    "effnet = EfficientNetB5(weights=None,\n",
    "                        include_top=False,\n",
    "                        input_shape=(IMG_WIDTH, IMG_HEIGHT, CHANNELS))\n",
    "effnet.load_weights('../input/efficientnet-keras-weights-b0b5/efficientnet-b5_imagenet_1000_notop.h5')\n",
    "\n",
    "def build_model(learning_rate):\n",
    "    \"\"\"\n",
    "    A custom implementation of EfficientNetB3\n",
    "    for the APTOS 2019 competition\n",
    "    (Regression with 5 classes)\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(effnet)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation = \"linear\"))\n",
    "    model.compile(loss='mse',\n",
    "                  optimizer=Adam(learning_rate), \n",
    "                  metrics=['mse', 'acc'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "learning_rate = 0.00005\n",
    "model = build_model(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,300,300,144] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node efficientnet-b5/batch_normalization_8/cond/Merge-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[metrics/acc/Mean_1/_9911]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,300,300,144] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node efficientnet-b5/batch_normalization_8/cond/Merge-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c32a69f0999d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                     callbacks=[kappa_metrics, es])\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,300,300,144] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node efficientnet-b5/batch_normalization_8/cond/Merge-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[metrics/acc/Mean_1/_9911]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,300,300,144] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node efficientnet-b5/batch_normalization_8/cond/Merge-0-0-TransposeNCHWToNHWC-LayoutOptimizer}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "kappa_metrics = Metrics()\n",
    "es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1, patience=6)\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=90,\n",
    "                    epochs=8,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps = 90,\n",
    "                    callbacks=[kappa_metrics, es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "90/90 [==============================] - 1061s 12s/step - loss: 3.2095 - mean_squared_error: 3.2095 - acc: 0.4447 - val_loss: 3.1588 - val_mean_squared_error: 3.1588 - val_acc: 0.4568\n",
      "val_kappa: 0.0147\n",
      "Validation Kappa has improved. Saving model.\n",
      "Epoch 2/12\n",
      "90/90 [==============================] - 945s 10s/step - loss: 3.0517 - mean_squared_error: 3.0517 - acc: 0.4667 - val_loss: 3.0863 - val_mean_squared_error: 3.0863 - val_acc: 0.4563\n",
      "val_kappa: 0.0134\n",
      "Epoch 3/12\n",
      "90/90 [==============================] - 950s 11s/step - loss: 2.9240 - mean_squared_error: 2.9240 - acc: 0.4725 - val_loss: 2.9988 - val_mean_squared_error: 2.9988 - val_acc: 0.4546\n",
      "val_kappa: -0.0003\n",
      "Epoch 4/12\n",
      "90/90 [==============================] - 951s 11s/step - loss: 2.8429 - mean_squared_error: 2.8429 - acc: 0.4657 - val_loss: 2.9203 - val_mean_squared_error: 2.9203 - val_acc: 0.4529\n",
      "val_kappa: 0.0019\n",
      "Epoch 5/12\n",
      "90/90 [==============================] - 987s 11s/step - loss: 2.7086 - mean_squared_error: 2.7086 - acc: 0.4914 - val_loss: 2.9208 - val_mean_squared_error: 2.9208 - val_acc: 0.4594\n",
      "val_kappa: -0.0039\n",
      "Epoch 6/12\n",
      "90/90 [==============================] - 999s 11s/step - loss: 2.7282 - mean_squared_error: 2.7282 - acc: 0.4775 - val_loss: 2.7831 - val_mean_squared_error: 2.7831 - val_acc: 0.4568\n",
      "val_kappa: 0.0041\n",
      "Epoch 7/12\n",
      "90/90 [==============================] - 996s 11s/step - loss: 2.5621 - mean_squared_error: 2.5621 - acc: 0.4841 - val_loss: 2.8049 - val_mean_squared_error: 2.8049 - val_acc: 0.4434\n",
      "val_kappa: 0.0058\n",
      "Epoch 8/12\n",
      "90/90 [==============================] - 977s 11s/step - loss: 2.6153 - mean_squared_error: 2.6153 - acc: 0.4683 - val_loss: 2.7773 - val_mean_squared_error: 2.7773 - val_acc: 0.4470\n",
      "val_kappa: -0.0066\n",
      "Epoch 9/12\n",
      "90/90 [==============================] - 936s 10s/step - loss: 2.4493 - mean_squared_error: 2.4493 - acc: 0.4874 - val_loss: 2.6850 - val_mean_squared_error: 2.6850 - val_acc: 0.4492\n",
      "val_kappa: 0.0061\n",
      "Epoch 10/12\n",
      "90/90 [==============================] - 923s 10s/step - loss: 2.5781 - mean_squared_error: 2.5781 - acc: 0.4645 - val_loss: 2.6967 - val_mean_squared_error: 2.6967 - val_acc: 0.4332\n",
      "val_kappa: 0.0288\n",
      "Validation Kappa has improved. Saving model.\n",
      "Epoch 11/12\n",
      "90/90 [==============================] - 906s 10s/step - loss: 2.3267 - mean_squared_error: 2.3267 - acc: 0.4876 - val_loss: 2.6618 - val_mean_squared_error: 2.6618 - val_acc: 0.4288\n",
      "val_kappa: 0.0170\n",
      "Epoch 12/12\n",
      "90/90 [==============================] - 911s 10s/step - loss: 2.2631 - mean_squared_error: 2.2631 - acc: 0.4775 - val_loss: 2.6146 - val_mean_squared_error: 2.6146 - val_acc: 0.4257\n",
      "val_kappa: 0.0056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f865b7fa940>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unfreeze layers and fine-tune model\n",
    "model.layers[0].trainable = False\n",
    "# Don't forget to compile again when you change trainable flags\n",
    "model.compile(loss='mse',\n",
    "              optimizer=Adam(0.00001), \n",
    "              metrics=['mse', 'acc'])\n",
    "\n",
    "# Start second training phase (fine-tune all layers)\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=90,\n",
    "                    epochs=12,\n",
    "                    validation_data=val_generator,\n",
    "                    validation_steps=90,\n",
    "                    callbacks=[kappa_metrics, es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate QWK on train set\n",
    "y_train_preds, train_labels = get_preds_and_labels(model, train_generator)\n",
    "y_train_preds = np.rint(y_train_preds).astype(np.uint8).clip(0, 4)\n",
    "\n",
    "# Calculate score\n",
    "train_score = cohen_kappa_score(train_labels, y_train_preds, weights=\"quadratic\")\n",
    "\n",
    "# Calculate QWK on validation set\n",
    "y_val_preds, val_labels = get_preds_and_labels(model, val_generator)\n",
    "y_val_preds = np.rint(y_val_preds).astype(np.uint8).clip(0, 4)\n",
    "\n",
    "# Calculate score\n",
    "val_score = cohen_kappa_score(val_labels, y_val_preds, weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedRounder(object):\n",
    "    \"\"\"\n",
    "    An optimizer for rounding thresholds\n",
    "    to maximize Quadratic Weighted Kappa score\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _kappa_loss(self, coef, X, y):\n",
    "        \"\"\"\n",
    "        Get loss according to\n",
    "        using current coefficients\n",
    "        \"\"\"\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "\n",
    "        ll = cohen_kappa_score(y, X_p, weights='quadratic')\n",
    "        return -ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Optimize rounding thresholds\n",
    "        \"\"\"\n",
    "        loss_partial = partial(self._kappa_loss, X=X, y=y)\n",
    "        initial_coef = [0.5, 1.5, 2.5, 3.5]\n",
    "        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        \"\"\"\n",
    "        Make predictions with specified thresholds\n",
    "        \"\"\"\n",
    "        X_p = np.copy(X)\n",
    "        for i, pred in enumerate(X_p):\n",
    "            if pred < coef[0]:\n",
    "                X_p[i] = 0\n",
    "            elif pred >= coef[0] and pred < coef[1]:\n",
    "                X_p[i] = 1\n",
    "            elif pred >= coef[1] and pred < coef[2]:\n",
    "                X_p[i] = 2\n",
    "            elif pred >= coef[2] and pred < coef[3]:\n",
    "                X_p[i] = 3\n",
    "            else:\n",
    "                X_p[i] = 4\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize on validation data and evaluate again\n",
    "y_val_preds, val_labels = get_preds_and_labels(model, val_generator)\n",
    "optR = OptimizedRounder()\n",
    "optR.fit(y_val_preds, val_labels)\n",
    "coefficients = optR.coefficients()\n",
    "opt_val_predictions = optR.predict(y_val_preds, coefficients)\n",
    "new_val_score = cohen_kappa_score(val_labels, opt_val_predictions, weights=\"quadratic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 1928/1928 [02:33<00:00, 12.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess test images\n",
    "N = test_df.shape[0]\n",
    "x_test = np.empty((N, IMG_WIDTH, IMG_HEIGHT, CHANNELS), dtype=np.uint8)\n",
    "for i, image_id in enumerate(tqdm(test_df['id_code'])):\n",
    "    x_test[i, :, :, :] = preprocess_image(f'{TEST_IMG_PATH}/{image_id}') / 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions, round predictions and save to csv\n",
    "y_test = model.predict(x_test)\n",
    "y_test = optR.predict(y_test, coefficients).astype(np.uint8)\n",
    "test_df['diagnosis'] = y_test\n",
    "test_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel runtime = 4.0224 hours (241 minutes)\n"
     ]
    }
   ],
   "source": [
    "# Check kernels run-time. GPU limit for this competition is set to Â± 9 hours.\n",
    "t_finish = time.time()\n",
    "total_time = round((t_finish-t_start) / 3600, 4)\n",
    "print('Kernel runtime = {} hours ({} minutes)'.format(total_time, \n",
    "                                                      int(total_time*60)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
